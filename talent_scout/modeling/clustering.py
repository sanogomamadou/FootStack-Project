# talent_scout/modeling/clustering.py
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import logging
import joblib
from datetime import datetime
import sys
import os

current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
sys.path.insert(0, root_dir)

logger = logging.getLogger(__name__)

# Imports pour la base de donn√©es
try:
    from data_ingest.db import SessionLocal
    from data_ingest.models import PlayerCluster
    print("‚úÖ Modules data_ingest import√©s avec succ√®s")
except ImportError as e:
    print(f"‚ùå Erreur import data_ingest: {e}")
    sys.exit(1)

class PlayerClustering:
    def __init__(self, n_clusters: int = 8, random_state: int = 42):
        self.n_clusters = n_clusters
        self.random_state = random_state
        self.scaler = StandardScaler()
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
        self.feature_columns = []
        
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Pr√©parer les features pour le clustering
        """
        logger.info("Pr√©paration des features pour le clustering...")
        
        # Features de base pour tous les joueurs (sauf gardiens)
        base_features = [
            'goals_per90', 'assists_per90', 'goal_contrib_per90', 
            'minutes_played_norm'
        ]
        
        # S'assurer que toutes les colonnes existent
        available_features = [col for col in base_features if col in df.columns]
        
        if len(available_features) < 2:
            raise ValueError("Pas assez de features disponibles pour le clustering")
            
        features_df = df[available_features].copy()
        
        # Nettoyer les valeurs infinies et NaN
        features_df = features_df.replace([np.inf, -np.inf], np.nan)
        features_df = features_df.fillna(0)
        
        # Filtrer les outliers extr√™mes (top 1%)
        for col in available_features:
            q99 = features_df[col].quantile(0.99)
            features_df[col] = np.where(features_df[col] > q99, q99, features_df[col])
        
        self.feature_columns = available_features
        logger.info(f"Features utilis√©es: {self.feature_columns}")
        
        return features_df
    
    def find_optimal_clusters(self, df: pd.DataFrame, max_k: int = 15) -> int:
        """
        Trouver le nombre optimal de clusters avec la m√©thode du coude
        """
        logger.info("Recherche du nombre optimal de clusters...")
        
        features_df = self.prepare_features(df)
        X_scaled = self.scaler.fit_transform(features_df)
        
        inertias = []
        silhouette_scores = []
        k_range = range(2, max_k + 1)
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)
            kmeans.fit(X_scaled)
            
            inertias.append(kmeans.inertia_)
            
            # Score de silhouette (√©viter quand k=1)
            if k > 1:
                silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))
            else:
                silhouette_scores.append(0)
        
        # M√©thode du coude simplifi√©e
        optimal_k = 8  # Valeur par d√©faut pour commencer
        
        # Trouver le "coude" dans la courbe d'inertie
        differences = []
        for i in range(1, len(inertias)):
            diff = inertias[i-1] - inertias[i]
            differences.append(diff)
        
        if differences:
            # Le coude est o√π la r√©duction d'inertie diminue significativement
            avg_reduction = np.mean(differences)
            for i, diff in enumerate(differences):
                if diff < avg_reduction * 0.5:  # Quand la r√©duction chute de moiti√©
                    optimal_k = i + 2  # +2 car on commence √† k=2
                    break
        
        logger.info(f"Nombre optimal de clusters sugg√©r√©: {optimal_k}")
        
        # Visualisation (optionnelle)
        self._plot_elbow_curve(k_range, inertias, silhouette_scores, optimal_k)
        
        return optimal_k
    
    def _plot_elbow_curve(self, k_range, inertias, silhouette_scores, optimal_k):
        """Visualiser la courbe du coude"""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
            
            # Courbe d'inertie
            ax1.plot(k_range, inertias, 'bo-')
            ax1.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)
            ax1.set_xlabel('Nombre de clusters')
            ax1.set_ylabel('Inertie')
            ax1.set_title('M√©thode du Coude')
            ax1.grid(True, alpha=0.3)
            
            # Score de silhouette
            ax2.plot(k_range[1:], silhouette_scores[1:], 'go-')
            ax2.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7)
            ax2.set_xlabel('Nombre de clusters')
            ax2.set_ylabel('Score Silhouette')
            ax2.set_title('Score de Silhouette')
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('data/clustering_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("üìä Graphique d'analyse sauvegard√©: data/clustering_analysis.png")
            
        except Exception as e:
            logger.warning(f"Impossible de sauvegarder le graphique: {e}")
    
    def fit_clusters(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Appliquer le clustering K-means
        """
        logger.info("Application du clustering K-means...")
        
        # Pr√©parer les features
        features_df = self.prepare_features(df)
        
        # Standardiser les features
        X_scaled = self.scaler.fit_transform(features_df)
        
        # Ajuster le mod√®le K-means
        self.kmeans.fit(X_scaled)
        
        # Ajouter les clusters au DataFrame original
        df_with_clusters = df.copy()
        df_with_clusters['cluster'] = self.kmeans.labels_
        
        # Calculer la distance au centro√Øde (mesure de typicalit√©)
        distances = self.kmeans.transform(X_scaled)
        df_with_clusters['distance_to_centroid'] = distances.min(axis=1)
        
        # Analyser les clusters
        cluster_analysis = self._analyze_clusters(df_with_clusters)
        
        logger.info(f"‚úÖ Clustering termin√©. {self.n_clusters} clusters cr√©√©s.")
        logger.info(f"üìà Distribution: {df_with_clusters['cluster'].value_counts().to_dict()}")
        
        return df_with_clusters, cluster_analysis
    
    def _analyze_clusters(self, df: pd.DataFrame) -> Dict:
        """
        Analyser les caract√©ristiques de chaque cluster
        """
        analysis = {}
        
        for cluster_id in sorted(df['cluster'].unique()):
            cluster_data = df[df['cluster'] == cluster_id]
            
            # Caract√©ristiques moyennes du cluster
            cluster_profile = {
                'size': len(cluster_data),
                'avg_goals_per90': cluster_data['goals_per90'].mean(),
                'avg_assists_per90': cluster_data['assists_per90'].mean(),
                'avg_goal_contrib_per90': cluster_data['goal_contrib_per90'].mean(),
                'avg_minutes_played': cluster_data['minutes_played'].mean(),
                'main_positions': cluster_data['position_group'].value_counts().head(3).to_dict(),
                'top_teams': cluster_data['team'].value_counts().head(3).to_dict()
            }
            
            analysis[cluster_id] = cluster_profile
        
        return analysis
    
    def find_undervalued_players(self, df: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:
        """
        Identifier les joueurs sous-√©valu√©s (haute performance mais dans des petits clusters)
        """
        logger.info("Recherche de joueurs sous-√©valu√©s...")
        
        # Calculer un score de performance simple
        df['performance_score'] = (
            df['goals_per90'] * 0.6 + 
            df['assists_per90'] * 0.4 +
            df['goal_contrib_per90'] * 0.3
        )
        
        # Normaliser le score de performance
        df['performance_score_norm'] = (
            (df['performance_score'] - df['performance_score'].min()) / 
            (df['performance_score'].max() - df['performance_score'].min())
        )
        
        # Identifier les clusters avec haute performance moyenne
        cluster_performance = df.groupby('cluster')['performance_score_norm'].mean()
        high_perf_clusters = cluster_performance[cluster_performance > cluster_performance.median()].index
        
        # Trouver les joueurs performants dans des petits clusters (potentiellement sous-√©valu√©s)
        undervalued_candidates = df[
            (df['cluster'].isin(high_perf_clusters)) &
            (df['performance_score_norm'] > 0.7)  # Hautement performants
        ].copy()
        
        # Trier par performance et distance au centro√Øde (plus atypique = plus int√©ressant)
        undervalued_candidates['undervalued_score'] = (
            undervalued_candidates['performance_score_norm'] * 0.7 +
            (1 - undervalued_candidates['distance_to_centroid']) * 0.3  # Plus proche du centro√Øde = plus typique
        )
        
        top_undervalued = undervalued_candidates.nlargest(top_n, 'undervalued_score')[
            ['name', 'team', 'position_group', 'goals_per90', 'assists_per90', 
             'performance_score_norm', 'cluster', 'distance_to_centroid', 'undervalued_score']
        ]
        
        logger.info(f"üéØ {len(top_undervalued)} joueurs sous-√©valu√©s identifi√©s")
        
        return top_undervalued
    
    def save_model(self, filepath: str = "models/player_clustering.joblib"):
        """Sauvegarder le mod√®le de clustering"""
        import os
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        model_data = {
            'kmeans': self.kmeans,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'n_clusters': self.n_clusters,
            'timestamp': datetime.now()
        }
        
        joblib.dump(model_data, filepath)
        logger.info(f"üíæ Mod√®le sauvegard√©: {filepath}")
    
    @classmethod
    def load_model(cls, filepath: str = "models/player_clustering.joblib"):
        """Charger un mod√®le de clustering sauvegard√©"""
        model_data = joblib.load(filepath)
        
        instance = cls(n_clusters=model_data['n_clusters'])
        instance.kmeans = model_data['kmeans']
        instance.scaler = model_data['scaler']
        instance.feature_columns = model_data['feature_columns']
        
        logger.info(f"üìÇ Mod√®le charg√©: {filepath}")
        return instance

    def save_clusters_to_database(self, df_clustered: pd.DataFrame):
        """
        Sauvegarder les r√©sultats du clustering dans la table player_clusters
        """
        logger.info("üíæ Sauvegarde des clusters en base de donn√©es...")
        
        session = SessionLocal()
        try:
            # Vider la table existante pour √©viter les doublons
            session.query(PlayerCluster).delete()
            
            # Ins√©rer les nouveaux clusters
            clusters_to_insert = []
            for _, row in df_clustered.iterrows():
                # V√©rifier que l'ID du joueur existe
                if pd.notna(row.get('id_info')):
                    cluster_record = PlayerCluster(
                        player_id=int(row['id_info']),
                        cluster_id=int(row['cluster']),
                        position_group=row.get('position_group', 'Unknown'),
                        similarity_score=float(1 - row.get('distance_to_centroid', 1.0)),
                        created_at=datetime.utcnow()
                    )
                    clusters_to_insert.append(cluster_record)
            
            # Insertion en batch
            session.bulk_save_objects(clusters_to_insert)
            session.commit()
            
            logger.info(f"‚úÖ {len(clusters_to_insert)} clusters sauvegard√©s en base de donn√©es")
            
        except Exception as e:
            session.rollback()
            logger.error(f"‚ùå Erreur sauvegarde clusters en base: {e}")
            raise
        finally:
            session.close()

def run_complete_clustering_pipeline():
    """
    Ex√©cuter le pipeline complet de clustering
    """
    logger.info("üöÄ D√©marrage du pipeline complet de clustering...")
    
    try:
        # MODIFICATION : Charger directement depuis le CSV existant
        logger.info("üìÇ Chargement des donn√©es depuis data/processed_players.csv...")
        df = pd.read_csv("data/processed_players.csv")
        logger.info(f"‚úÖ Donn√©es charg√©es: {len(df)} joueurs")
        
    except FileNotFoundError:
        # Fallback : utiliser le data_processor si le CSV n'existe pas
        logger.info("üìÇ Fichier CSV non trouv√©, utilisation du data_processor...")
        try:
            from talent_scout.data_collection.data_processor import process_player_data
            df = process_player_data(save_csv=True)
        except ImportError as e:
            logger.error(f"‚ùå Impossible d'importer data_processor: {e}")
            logger.error("üí° Assure-toi que le fichier data/processed_players.csv existe")
            return None, None, None
    
    if df.empty:
        logger.error("‚ùå Aucune donn√©e √† clusteriser")
        return None, None, None
    
    # 2. Initialiser et configurer le clustering
    cluster_analyzer = PlayerClustering(n_clusters=8)
    
    # 3. Trouver le nombre optimal de clusters
    optimal_k = cluster_analyzer.find_optimal_clusters(df)
    cluster_analyzer.n_clusters = optimal_k
    
    # 4. Appliquer le clustering
    df_clustered, cluster_analysis = cluster_analyzer.fit_clusters(df)
    
    # 5. Identifier les joueurs sous-√©valu√©s
    undervalued_players = cluster_analyzer.find_undervalued_players(df_clustered, top_n=15)
    
    # 6. Sauvegarder les r√©sultats
    df_clustered.to_csv("data/players_with_clusters.csv", index=False)
    cluster_analyzer.save_model()
    
    # 7. NOUVEAU : Sauvegarder en base de donn√©es
    cluster_analyzer.save_clusters_to_database(df_clustered)
    
    # 8. Afficher les r√©sultats
    print("\n" + "="*70)
    print("üéØ RAPPORT DE CLUSTERING - JOUEURS SOUS-√âVALU√âS")
    print("="*70)
    
    print(f"\nüèÜ TOP 15 JOUEURS SOUS-√âVALU√âS:")
    for idx, (_, player) in enumerate(undervalued_players.iterrows(), 1):
        print(f"{idx:2d}. {player['name']:25} | {player['team']:20} | "
              f"{player['position_group']:10} | Buts/90: {player['goals_per90']:4.2f} | "
              f"Passes/90: {player['assists_per90']:4.2f} | Score: {player['undervalued_score']:.3f}")
    
    print(f"\nüìä ANALYSE DES {optimal_k} CLUSTERS:")
    for cluster_id, profile in cluster_analysis.items():
        main_positions = list(profile['main_positions'].keys())[:2] if profile['main_positions'] else []
        positions_str = ", ".join(main_positions)
        print(f"   Cluster {cluster_id}: {profile['size']:3} joueurs | "
              f"Buts/90: {profile['avg_goals_per90']:5.2f} | "
              f"Passes/90: {profile['avg_assists_per90']:5.2f} | "
              f"Positions: {positions_str}")
    
    print(f"\nüìà STATISTIQUES GLOBALES:")
    print(f"   ‚Ä¢ Total joueurs clusteris√©s: {len(df_clustered)}")
    print(f"   ‚Ä¢ Joueurs sous-√©valu√©s identifi√©s: {len(undervalued_players)}")
    print(f"   ‚Ä¢ Score moyen des sous-√©valu√©s: {undervalued_players['undervalued_score'].mean():.3f}")
    
    # V√©rification base de donn√©es
    try:
        session = SessionLocal()
        cluster_count = session.query(PlayerCluster).count()
        session.close()
        print(f"   ‚Ä¢ Clusters en base de donn√©es: {cluster_count}")
    except Exception as e:
        print(f"   ‚Ä¢ ‚ùå Erreur v√©rification base: {e}")
    
    print(f"\nüí° INTERPR√âTATION:")
    print("   ‚Ä¢ Score > 0.8: Tr√®s sous-√©valu√© - forte recommandation")
    print("   ‚Ä¢ Score 0.6-0.8: Sous-√©valu√© - bonne opportunit√©") 
    print("   ‚Ä¢ Score < 0.6: Potentiel √† surveiller")
    
    return df_clustered, undervalued_players, cluster_analysis

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    print("üîß Initialisation du clustering des joueurs...")
    df_clustered, undervalued_players, cluster_analysis = run_complete_clustering_pipeline()
    
    if df_clustered is not None:
        print("\n‚úÖ Clustering termin√© avec succ√®s!")
        print("üíæ Fichiers cr√©√©s:")
        print("   - data/players_with_clusters.csv")
        print("   - data/clustering_analysis.png") 
        print("   - models/player_clustering.joblib")
    else:
        print("\n‚ùå Le clustering a √©chou√©")